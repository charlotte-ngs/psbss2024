---
title: Model Selection and Variance Components
author: Peter von Rohr
date: "`r Sys.Date()`"
output: beamer_presentation
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::knit_hooks$set(hook_convert_odg = rmdhelp::hook_convert_odg)
```


## Breeding Programs

```{r bprogdiag, echo=FALSE, hook_convert_odg=TRUE, fig_path="odg", fig.pos='H', out.width='11cm'}
#rmddochelper::use_odg_graphic(ps_path = "odg/bprogdiag.odg")
knitr::include_graphics(path = "odg/bprogdiag.png")
```


## New Trait

* New trait to be considered in breeding program
* Why? $\rightarrow$ Trait is of economic importance
* Want to improve average level of trait in a given population 
* How is this done?
* What do we have to do?


## Background and Context

* Farms/Enterprise use livestock products as base for economic existence
* Improvements of production efficiency improves sustainability 
* Short-term: 
    + improve management and environment
    + select optimal livestock breed / population for given environment
* Long-term: 
    + improve population at genetic level
    + define breeding goal
    + select parents such that offspring are "closer" to goal compared to parents


## Genetic Improvement

* Genetic improvement happens between parents and offspring
* Parents pass random sample of alleles to offspring
* Goal: select parents that have many "good" alleles to pass to offspring
* How to find parents with "good" alleles without knowing which genes are important?

$\rightarrow$ __Statistical Modeling__


## Why Statistical Modelling?

Some people believe, they do not need statistics. For them it is enough to look at a diagram

```{r no-stat-model-diagram, echo=FALSE, hook_convert_odg=TRUE, fig_path="odg", out.height="7cm"}
#rmdhelp::use_odg_graphic("odg/no-stat-model-diagram.odg")
knitr::include_graphics(path = "odg/no-stat-model-diagram.png")
```


## Statistical Modelling Because ...
Two types of dependencies between physical quantities

1. deterministic
2. stochastic


## Deterministic Versus Stochastic

```{r deter-vs-stoch, echo=FALSE, hook_convert_odg=TRUE, fig_path="odg"}
#rmdhelp::use_odg_graphic(ps_path = "odg/deter-vs-stoch.odg")
knitr::include_graphics(path = "odg/deter-vs-stoch.png")
```

## Statistical Model

* stochastic systems contains many sources of uncertainty
* statistical models can handle uncertainty
* components of a statistical model
    + response variable $y$
    + predictor variables $x_1, x_2, \ldots, x_k$
    + error term $e$
    + function $m(x)$ 


## How Does A Statistical Model Work?

* predictor variables $x_1, x_2, \ldots, x_k$ are transformed by function $m(x)$ to explain the response variable $y$
* uncertainty is captured by error term.
* as a formula, for observation $i$

$$y_i = m(x_i) + e_i$$


## Which function $m(x)$?

* class of functions that can be used as $m(x)$ is infinitely large
* restrict to linear functions of predictor variables


## Which predictor variables?

* Question, about which predictor variables to use is answered by model selection

## Why Model Selection

* Many predictor variables are available
* Are all of them `relevant`?
* What is the meaning of `relevant` in this context?


## Example Dataset
```{r}
suppressPackageStartupMessages( require(dplyr) )
tbl_reg <- tibble::tibble(`Breast Circumference` = c(176, 177, 178, 179, 179, 180, 181, 182,183, 184),
                          `Body Weight` = c(471, 463, 481, 470, 496, 491, 518, 511, 510, 541))
n_nr_ani <- nrow(tbl_reg)
tbl_reg <- bind_cols(tibble::tibble(Animal = 1:n_nr_ani),
                     tbl_reg)
vec_randpred <- runif(n_nr_ani, 
                      min = min(tbl_reg$`Breast Circumference`), 
                      max = max(tbl_reg$`Breast Circumference`))
tbl_reg_aug <- bind_cols(tbl_reg, tibble(RandPred = round(vec_randpred, digits = 0)))
knitr::kable(tbl_reg_aug)
```


## No Relevance of Predictors

```{r}
plot(tbl_reg_aug$RandPred, tbl_reg_aug$`Body Weight`)
```


## Relevance of Predictors

```{r}
plot(tbl_reg_aug$`Breast Circumference`, tbl_reg_aug$`Body Weight`)
```


## Fitting a Regression Model 
\scriptsize
```{r}
lm_randpred <- lm(`Body Weight` ~ RandPred, data = tbl_reg_aug)
summary(lm_randpred)
```


## Fitting a Regression Model II
\scriptsize
```{r}
lm_bc <- lm(`Body Weight` ~ `Breast Circumference`, data = tbl_reg_aug)
summary(lm_bc)
```


## Multiple Regression
\scriptsize
```{r}
lm_full <- lm(`Body Weight` ~ `Breast Circumference` + RandPred, data = tbl_reg_aug)
summary(lm_full)
```


## Which model is better?
Why not taking all predictors?

* Additional parameters must be estimated from data
* Predictive power decreased with too many predictors (cannot be shown for this data set, because too few data points)
* Bias-variance trade-off


## Bias-variance trade-off
* Assume, we are looking for optimum prediction

$$s_i = \sum_{r=1}^q \hat{\beta}_{j_r}x_{ij_r}$$
with $q$ relevant predictor variables

* Average mean squared error of prediction $s_i$ 

$$MSE = n^{-1}\sum_{i=1}^n E\left[(m(x_i) - s_i)^2 \right]$$
where $m(.)$ denotes the linear function of the unknown true model. 

## Bias-variance trade-off II

* MSE can be split into two parts

$$MSE = n^{-1}\sum_{i=1}^n \left( E\left[s_i \right] - m(x_i) \right)^2 
  + n^{-1}\sum_{i=1}^n var(s_i)
$$

where $n^{-1}\sum_{i=1}^n \left( E\left[s_i \right] - m(x_i) \right)^2$ is called the squared __bias__

* Increasing $q$ leads to reduced bias but increased variance ($var(s_i)$)
* Hence, find $s_i$ such that MSE is minimal
* Problem: cannot compute MSE because $m(.)$ is not known 

$\rightarrow$ estimate MSE


## Mallows $C_p$ statistic
* For a given model $\mathcal{M}$, $SSE(\mathcal{M})$ stands for the residual sum of squares.
* MSE can be estimated as

$$\widehat{MSE} = n^{-1} SSE(\mathcal{M}) - \hat{\sigma}^2 + 2 \hat{\sigma}^2 |\mathcal{M}|/n$$

where $\hat{\sigma}^2$ is the estimate of the error variance of the full model, $SSE(\mathcal{M})$ is the residual sum of squares of the model $\mathcal{M}$, $n$ is the number of observations and $|\mathcal{M}|$ stands for the number of predictors in $\mathcal{M}$

$$C_p(\mathcal{M}) = \frac{SSE(\mathcal{M})}{\hat{\sigma}^2} - n + 2 |\mathcal{M}|$$


## Searching The Best Model
```{r}
n_nr_pred <- 16
```

* Exhaustive search over all sub-models might be too expensive
* For $p$ predictors there are $2^p - 1$ sub-models
* With $p=`r n_nr_pred`$, we get $`r 2^n_nr_pred - 1`$ sub-models

$\rightarrow$ step-wise approaches

## Forward Selection
1. Start with smallest sub-model $\mathcal{M}_0$ as current model
2. Include predictor that reduces SSE the most to current model
3. Repeat step 2 until all predictors are chosen

$\rightarrow$ results in sequence $\mathcal{M}_0 \subseteq \mathcal{M}_1 \subseteq \mathcal{M}_2 \subseteq \ldots$ of sub-models

4. Out of sequence of sub-models choose the one with minimal $C_p$


## Backward Selection
1. Start with full model $\mathcal{M}_0$ as the current model
2. Exclude predictor variable that increases SSE the least from current model
3. Repeat step 2 until all predictors are excluded (except for intercept)

$\rightarrow$ results in sequence $\mathcal{M}_0 \supseteq \mathcal{M}_1 \supseteq \mathcal{M}_2 \supseteq \ldots$ of sub-models

4. Out of sequence choose the one with minimal $C_p$


## Considerations
* Whenever possible, choose __backward__ selection, because it leads to better results
* If $p \geq n$, only forward is possible, but then consider LASSO


## Alternative Selection Criteria
* AIC or BIC, requires distributional assumptions. 
* AIC is implemented in `MASS::stepAIC()` 
* Adjusted $R^2$ is a measure of goodness of fit, but sometimes is not conclusive when comparing two models
* Try in exercise 


## Genetic Variation

* Requirement for trait to be considered in breeding goal
* Breeding means improvement of next generation via selection and mating
* Only genetic (additive) components are passed to offspring
* Selection should be based on genetic component of trait
* Selection only possible with genetic variation

$\rightarrow$ genetic variation indicates how good characteristics are passed from parents to offspring

$\rightarrow$ measured by __heritability__ $h^2 = \frac{\sigma_a^2}{\sigma_p^2}$


## Two Traits

```{r preparednormplot, eval=FALSE}
# according to https://sebastiansauer.github.io/normal_curve_ggplot2/
require(ggplot2)
p1 <- ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) + ylab("") +
  scale_y_continuous(breaks = NULL) + 
  theme_bw()
p1
```

```{r twotraitwithwoutgv, echo=FALSE, hook_convert_odg=TRUE, fig_path="odg", out.width="10cm"}
#rmddochelper::use_odg_graphic(ps_path = "odg/twotraitwithwoutgv.odg")
knitr::include_graphics(path = "odg/twotraitwithwoutgv.png")
```
 
 

## Problems

* Genetic components cannot be observed or measured
* Must be estimated from data
* Data are mostly phenotypic

$\rightarrow$ topic of variance components estimation

* Model based, that means connection between phenotypic measure and genetic component are based on certain model

$$p = g + e$$

with $cov(g,e) = 0$ 

* __Goal__: separate variation due to $g$ ($\sigma_a^2$) from phenotypic variation


## Example of Variance Components Separation 

* Estimation of repeatability
* Given repeated measurements of same trait at the same animal
* Repeatability means variation of measurements at the same animal is smaller than variation between measurements at different animals


## Repeatability Plot

```{r repmeasurebulldef, echo=FALSE, message=FALSE, warning=FALSE}
n_nr_bull <- 10
tbl_repmeasure_bull <- tibble::tibble(Bull = c(1:n_nr_bull),
                                      M1 = c(135, 129, 135, 127, 126, 128, 127, 129, 126, 132),
                                      M2 = c(136, 130, 133, 127, 129, 129, 132, 128, 125, 131),
                                      M3 = c(134, 128, 136, 125, 129, 128, 130 ,125, 127, 134))
tbl_rm_bull_ga <- tidyr::gather(tbl_repmeasure_bull, "Measurement", "Height", -Bull)
tbl_rm_bull_ga$Measurement <- as.factor(tbl_rm_bull_ga$Measurement)
tbl_rm_bull_ga$Bull <- as.factor(tbl_rm_bull_ga$Bull)
# ggplot2::ggplot(Bull, Height, data = tbl_rm_bull_ga, colour = Measurement)
library(ggplot2)
ggplot(tbl_rm_bull_ga, aes(x = Bull, y = Height, group = Measurement)) + 
  geom_point(aes(color = Measurement))
```


## Model

\begin{equation}
y_{ij} = \mu + t_i + \epsilon_{ij} \notag
\end{equation}

\begin{tabular}{lll}
where  &  &  \\
       & $y_{ij}$         &  measurement $j$ of animal $i$    \\
       & $\mu$            &  expected value of $y$            \\
       & $t_i$            &  deviation of $y_{ij}$ from $\mu$ attributed to animal $i$ \\
       & $\epsilon_{ij}$  &  measurement error
\end{tabular}


## Estimation Of Variance Components

* $E(t_i) = 0$
* $\sigma_t^2 = E(t_i^2)$: variance component of total variance ($\sigma_y^2$) which can be attributed to the $t$-effects
* $E(\epsilon_{ij}) = 0$
* $\sigma_{\epsilon}^2 = E(\epsilon_{ij}^2)$: variance component attributed to $\epsilon$-effects
* $\sigma_y^2 = \sigma_t^2 + \sigma_{\epsilon}^2$

* Repeatability $w$ defined as:

\begin{equation}
w = \frac{\sigma_t^2}{\sigma_t^2 + \sigma_{\epsilon}^2} \notag
\end{equation}

$\rightarrow$ estimate of $\sigma_t^2$ needed


## Analysis Of Variance (ANOVA)

\begin{center}
\begin{tabular}{l|r|r|r|r}
Effect                 &  df      &  Sum Sq             &  Mean Sq                    & $E(Mean\ Sq)$ \\
\hline
Bull     $(t)$         &  $r-1$   &  $SSQ(t)$           &  $SSQ(t) / (r-1)$           &  $\sigma_{\epsilon}^2 + n * \sigma_t^2$ \\
Residual $(\epsilon)$  &  $N-r$   &  $SSQ(\epsilon)$    &  $SSQ(\epsilon) / (N-r)$    &  $\sigma_{\epsilon}^2$
\end{tabular}
\end{center}

where 

<!-- Essl1987 p. 212 -->

$$SSQ(t) = \left[ {1 \over n} \sum_{i=1}^r \left( \sum_{j=1}^n y_{ij}\right)^2 \right] - \left(\sum_{i=1}^r\sum_{j=1}^{n}y_{ij}\right)^2 / N$$ 

$$SSQ(\epsilon) =  \sum_{i=1}^r\sum_{j=1}^{n}y_{ij}^2 - \left[ {1 \over n} \sum_{i=1}^r \left( \sum_{j=1}^n y_{ij}\right)^2 \right]$$


## Zahlenbeispiel

\scriptsize

```{r gevcerepaov, echo=FALSE}
lm_bull_height <- lm(Height ~ Bull, data = tbl_rm_bull_ga)
aov_bull_height <- aov(lm_bull_height)
(summary_bull_height <- summary(aov_bull_height))
```

\normalsize

```{r extractmeansq, echo=FALSE}
# according to https://stat.ethz.ch/pipermail/r-help/2011-February/267554.html
vec_mean_sq <- summary_bull_height[[1]]$'Mean Sq'
vec_df <- summary_bull_height[[1]]$Df
n_nr_rep <- length(levels(tbl_rm_bull_ga$Measurement))
n_hatsigmat <- (vec_mean_sq[1] - vec_mean_sq[2]) / n_nr_rep
n_hat_repeat <- n_hatsigmat / (n_hatsigmat + vec_mean_sq[2])
```

Setting expected values of `Mean Sq` equal to estimates of variance components

$$\hat{\sigma}_{\epsilon}^2 = `r vec_mean_sq[2]` \text{ and } \hat{\sigma}_t^2 = \frac{`r round(vec_mean_sq[1], 2)` - `r vec_mean_sq[2]`}{`r n_nr_rep`} = `r round(n_hatsigmat, 2)`$$

Repeatability 

$$\hat{w} = \frac{\hat{\sigma}_t^2}{\hat{\sigma}_t^2 + \hat{\sigma}_{\epsilon}^2} = `r round(n_hat_repeat, 2)`$$


## Same Strategy for Sire Model

* Sire model is a mixed linear effects model with sire effects $s$ as random components

\begin{equation}
y = Xb + Zs + e \notag
\end{equation}

* In case where sires are not related, $var(s) = $I * \sigma_s^2$
* From $\sigma_s^2$, we get genetic additive variance as $\sigma_a^2 = 4*\sigma_s^2$


## ANOVA

\scriptsize

\begin{center}
\begin{tabular}{l|rrr|r}
Effect          &  Degrees of Freedom  &  Sum Sq      &  Mean Sq             & $E(Mean\ Sq)$ \\
\hline
Sire $(s|b)$    &  $r-1$               &  $SSQ(s|b)$  &  $SSQ(s|b) / (r-1)$  &  $\sigma_e^2 + k * \sigma_s^2$ \\
Residual $(e)$  &  $N-r$               &  $SSQ(e)$    &  $SSQ(e) / (N-r)$    &  $\sigma_e^2$
\end{tabular}
\end{center}

\normalsize

with $$k = \frac{1}{r-1} \left[ N - \frac{\sum_{i=1}^{r}n_i^2}{N} \right]$$


## Maximum Likelihood (ML)

* Likelihood 

\begin{equation}
L(\theta) = f(y | \theta) \notag
\end{equation}

* Normal distribution

\begin{equation}
L(\theta) = (2\pi)^{-1/2 n} \sigma^{-n}|H|^{-1/2} * exp\left\{-\frac{1}{2\sigma^2}(y-Xb)^T H^{-1} (y-Xb) \right\} \notag
\end{equation}

with $var(y) = H*\sigma^2$ and $\theta^T = \left[\begin{array}{cc} b  &  \sigma^2 \end{array} \right]$


## Maximization of Likelihood

* Set $\lambda = log L$
* Compute partial derivatives of $\lambda$ with respect to all unknowns

$$\frac{\partial \lambda}{\partial b}$$

$$\frac{\partial \lambda}{\partial \sigma^2}$$

* Set partial derivatives to $0$ and solve for unknowns
* Use solutions as estimates


## Restricted Maximum Likelihood (REML)

* Problem with ML: estimate of $\sigma^2$ depends on $b$ $\rightarrow$ undesirable
* Do transformations $Sy$ and $Qy$ 

(i) The matrix $S$ has rank $n-t$ and the matrix $Q$ has rank $t$ 
(ii) The result of the two transformations are independent, that means $cov(Sy,Qy) = 0$ which is met when $SHQ^T = 0$
(iii) The matrix S is chosen such that $E(Sy) = 0$ which means $SX = 0$
(iv) The matrix $QX$ is of rank $t$, so that every linear function of the elements of $Qy$ estimate a linear function of $b$. 


## REML II

* From (i) and (ii) it follows that the likelihood $L$ of $y$ is the product of the likelihoods of $Sy$ ($L^*$) and $Qy$ ($L^{**}$) that means 

$$\lambda = \lambda^* + \lambda^{**}$$

* Variance components are estimated from $\lambda^*$ which will then be independent of $b$


## Bayesian Estimation

* Proposed already in the 80's
* Full implementation only in 1993
* Requirements: 
    + cheap computing and 
    + good pseudo-random number generators
* Bayesian estimation is based on conditional posterior distribution of unknowns given the knowns
* Conditional posterior distribution is computed from prior distribution of unknowns times the likelihood
    


## Model

* Univariate Gaussian linear mixed model

$$y = Xb + Zu + e$$

\begin{tabular}{llp{6cm}}
where  & & \\
       &  $y$  &  vector of observations (length $n$) \\
       &  $b$  &  vector of fixed effects (length $p$) \\
       &  $u$  &  vector of random breeding values (length $q$) \\
       &  $e$  &  vector of random residuals (length $n$) \\
       &  $X$  &  $n\times p$ design matrix linking fixed effects to observations \\
       &  $Z$  &  $n\times q$ design matrix linking breeding values to observations
\end{tabular}


## Likelihood

* Data generating distribution

$$y | b, u, \sigma_e^2 \sim \mathcal{N}(Xb + Zu, I * \sigma_e^2)$$

where $I$ is a $n\times n$ identity matrix and $\sigma_e^2$ is the variance of the random residuals.


## Priors

* Prior distributions must be specified for all unknowns
* Unknowns in our example are: $b$, $u$, $\sigma_e^2$ and $\sigma_u^2$
* Prior distribution for 
    + $b$ is flat, i.e. $p(b) \propto c$
    + $u$ Normal distribution as $u | G, \sigma_u^2 \sim N(0, G*\sigma_u^2)$
    + $\sigma_e^2$ scaled inverse $\chi^2$: $p(\sigma_e^2 | \nu_e, s_e^2) \propto (\sigma_e^2)^{-\nu_e/2-1} exp(-{1\over 2}\nu_e s_e^2 / \sigma_e^2)$
    + $\sigma_u^2$ : $p(\sigma_u^2 | \nu_u, s_u^2) \propto (\sigma_u^2)^{-\nu_u/2-1} exp(-{1\over 2}\nu_u s_u^2 / \sigma_u^2)$
* $\nu_e$, $\nu_s$, $s_e^2$ and $s_u^2$ are called hyper-parameters and must be determined


## Additional Terms

*  Let

$$\theta^T = (b^T, u^T) = (\theta_1, \theta_2, \ldots, \theta_N)$$

$$\theta_{-i} = (\theta_1, \theta_2, \ldots, \theta_{i-1}, \theta_{i+1}, \ldots, \theta_N)$$

* Further, let

$$s^T = (s_u^2, s_e^2)$$
and

$$\nu^T = (\nu_u, \nu_e)$$

## Joint Posterior Density

The joint posterior distribution can be written as

$$p(\theta, \sigma_u^2, \sigma_e^2 | y, s, \nu) \propto p(\theta) * p(\sigma_u^2 | \nu_u, s_u^2) * p(\sigma_e^2 | \nu_e, s_e^2) * p(y | \theta, \sigma_e^2)$$


## Fully Conditional Posterior Densities of $\theta$

* Density of every single unknown component when setting all other components as known

$$\theta_i | y, \theta_{-i}, \sigma_u^2, \sigma_e^2, s, \nu \sim \mathcal{N}(\tilde{\theta_i}, \tilde{v_i}) $$

where $\tilde{\theta_i} = (r_i - \sum_{j=1,j\ne i}^N w_{ij}\theta_j)/w_{ii}$ and $\tilde{v_i} = \sigma_e^2 / w_{ii}$. 

* vector $r$ is the vector of right-hand side of MME
* matrix $W$ is the coefficient matrix of MME


## Fully Conditional Posterior Densities of $\sigma_e^2$

* scaled inverted chi-square distribution for $\sigma_e^2$

$$\sigma_e^2 | y, \theta, \sigma_u^2, s, \nu \sim \tilde{\nu_e}\tilde{s_e}^2\chi_{\tilde{\nu_e}}^{-2}$$

* Parameters of the above distribution are defined as 

$$\tilde{\nu_e} = n + \nu_e$$
and 

$$\tilde{s_e}^2 = \left[(y - Xb - Zu)^T(y - Xb - Zu) + \nu_e s_e^2 \right] / \tilde{\nu_e}$$


## Fully Conditional Posterior Densities of $\sigma_u^2$

* scaled inverted chi-square distribution for $\sigma_u^2$

$$\sigma_u^2 | y, \theta, \sigma_e^2, s, \nu \sim \tilde{\nu_u}\tilde{s_u}^2\chi_{\tilde{\nu_u}}^{-2}$$

* Parameters of the above distribution are defined as 

$$\tilde{\nu_u} = q + \nu_u$$
and 

$$\tilde{s_u}^2 = \left[u^TG^{-1}u + \nu_u s_u^2 \right] / \tilde{\nu_u}$$


## Implementation

* Step 1: set starting values for $\theta$, $\sigma_e^2$ and $\sigma_u^2$
* Step 2: draw random number for each component $\theta_i$ of $\theta$ from fully conditional distribution $\mathcal{N}(\tilde{\theta_i}, \tilde{v_i})$
* Step 3: draw random number for $\sigma_e^2$ from $\tilde{\nu_e}\tilde{s_e}^2\chi_{\tilde{\nu_e}}^{-2}$
* Step 4: draw random number for $\sigma_u^2$ from $\tilde{\nu_u}\tilde{s_u}^2\chi_{\tilde{\nu_u}}^{-2}$
* Repeat steps 2-4 many times and store random numbers
* Step 5: compute means of random numbers to get Bayesian estimates of unknowns $\theta$, $\sigma_e^2$ and $\sigma_u^2$
