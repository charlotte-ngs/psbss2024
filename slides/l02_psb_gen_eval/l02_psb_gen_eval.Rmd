---
title: Genetic Evaluation
author: Peter von Rohr
date: "2024-05-15"
output: beamer_presentation
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::knit_hooks$set(hook_convert_odg = rmdhelp::hook_convert_odg)
```


## Breeding Program

```{r bprogdiag, echo=FALSE, hook_convert_odg=TRUE, fig_path="odg", fig.pos='H', out.width='11cm'}
#rmddochelper::use_odg_graphic(ps_path = "odg/bprogdiag.odg")
knitr::include_graphics(path = "odg/bprogdiag.png")
```


## New Trait

* New trait to be considered in breeding program
* Why? $\rightarrow$ Trait is of economic importance
* Want to improve average level of trait in a given population 
* How is this done?
* What do we have to do?


## Background and Context

* Farms/Enterprise use livestock products as base for economic existence
* Improvements of production efficiency improves sustainability 
* Short-term: 
    + improve management and environment
    + select optimal livestock breed / population for given environment
* Long-term: 
    + improve population at genetic level
    + define breeding goal
    + select parents such that offspring are "closer" to goal compared to parents


## Genetic Improvement

* Genetic improvement happens between parents and offspring
* Parents pass random sample of alleles to offspring
* Goal: select parents that have many "good" alleles to pass to offspring
* Value of alleles quantified by breeding value
* How to find parents with "good" alleles without knowing which genes are important?

$\rightarrow$ Predict breeding value using __Statistical Modeling__


## Genotype and Phenotype

```{r geno_and_pheno, echo=FALSE, hook_convert_odg=TRUE, fig_path="odg", out.width="80%"}
#rmdhelp::use_odg_graphic(ps_path = "odg/geno_and_pheno.odg")
knitr::include_graphics(path = "odg/geno_and_pheno.png")
```

* Selection based on phenotypes: in-efficient
* Instead: use statistical model to predict breeding value


## Statistical Model

* stochastic systems contains many sources of uncertainty
* statistical models can handle uncertainty
* components of a statistical model
    + response variable $y$
    + predictor variables $x_1, x_2, \ldots, x_k$
    + error term $e$
    + function $m(x)$ 


## How Does A Statistical Model Work?

* predictor variables $x_1, x_2, \ldots, x_k$ are transformed by function $m(x)$ to explain the response variable $y$
* uncertainty is captured by error term.
* as a formula, for observation $i$

$$y_i = m(x_i) + e_i$$


## Which function $m(x)$?

* class of functions that can be used as $m(x)$ is infinitely large
* restrict to linear functions of model parameter ($b_0$ and $b_1$), e.g.

$$y_i = b_0 + b_1 * x_i + e_i$$


## Which predictor variables?

* In genetic evaluation large variety of information can be used as predictors
* Question, about which predictor variables to use is answered by model selection


## Why Model Selection

* Many predictor variables are available
* Are all of them `relevant`?
* What is the meaning of `relevant` in this context?


## Example Dataset
```{r}
set.seed(2404)
suppressPackageStartupMessages( require(dplyr) )
tbl_reg <- tibble::tibble(`Breast Circumference` = c(176, 177, 178, 179, 179, 180, 181, 182,183, 184),
                          `Body Weight` = c(471, 463, 481, 470, 496, 491, 518, 511, 510, 541))
n_nr_ani <- nrow(tbl_reg)
tbl_reg <- bind_cols(tibble::tibble(Animal = 1:n_nr_ani),
                     tbl_reg)
vec_randpred <- runif(n_nr_ani, 
                      min = min(tbl_reg$`Breast Circumference`), 
                      max = max(tbl_reg$`Breast Circumference`))
tbl_reg_aug <- bind_cols(tbl_reg, tibble(RandPred = round(vec_randpred, digits = 0)))
# write data
s_data_out <- here::here("docs", "data", "psb_rand_pred.csv")
readr::write_delim(tbl_reg_aug, s_data_out, delim = ",")
knitr::kable(tbl_reg_aug)
```


## No Relevance of Predictors

```{r}
plot(tbl_reg_aug$RandPred, tbl_reg_aug$`Body Weight`)
```


## Relevance of Predictors

```{r}
plot(tbl_reg_aug$`Breast Circumference`, tbl_reg_aug$`Body Weight`)
```


## Fitting a Regression Model 
\scriptsize
```{r}
lm_randpred <- lm(`Body Weight` ~ RandPred, data = tbl_reg_aug)
summary(lm_randpred)
```


## Fitting a Regression Model II
\scriptsize
```{r}
lm_bc <- lm(`Body Weight` ~ `Breast Circumference`, data = tbl_reg_aug)
summary(lm_bc)
```


## Multiple Regression
\scriptsize
```{r}
lm_full <- lm(`Body Weight` ~ `Breast Circumference` + RandPred, data = tbl_reg_aug)
summary(lm_full)
```


## Which model is better?
Why not taking all predictors?

* Additional parameters must be estimated from data
* Predictive power decreased with too many predictors (cannot be shown for this data set, because too few data points)
* Bias-variance trade-off


## Bias-variance trade-off
* Assume, we are looking for optimum prediction

$$s_i = \sum_{r=1}^q \hat{\beta}_{j_r}x_{ij_r}$$
with $q$ relevant predictor variables

* Average mean squared error of prediction $s_i$ 

$$MSE = n^{-1}\sum_{i=1}^n E\left[(m(x_i) - s_i)^2 \right]$$
where $m(.)$ denotes the linear function of the unknown true model. 

## Bias-variance trade-off II

* MSE can be split into two parts

$$MSE = n^{-1}\sum_{i=1}^n \left( E\left[s_i \right] - m(x_i) \right)^2 
  + n^{-1}\sum_{i=1}^n var(s_i)
$$

where $n^{-1}\sum_{i=1}^n \left( E\left[s_i \right] - m(x_i) \right)^2$ is called the squared __bias__

* Increasing $q$ leads to reduced bias but increased variance ($var(s_i)$)
* Hence, find $s_i$ such that MSE is minimal
* Problem: cannot compute MSE because $m(.)$ is not known 

$\rightarrow$ estimate MSE


## Mallows $C_p$ statistic
* For a given model $\mathcal{M}$, $SSE(\mathcal{M})$ stands for the residual sum of squares.
* MSE can be estimated as

$$\widehat{MSE} = n^{-1} SSE(\mathcal{M}) - \hat{\sigma}^2 + 2 \hat{\sigma}^2 |\mathcal{M}|/n$$

where $\hat{\sigma}^2$ is the estimate of the error variance of the full model, $SSE(\mathcal{M})$ is the residual sum of squares of the model $\mathcal{M}$, $n$ is the number of observations and $|\mathcal{M}|$ stands for the number of predictors in $\mathcal{M}$

$$C_p(\mathcal{M}) = \frac{SSE(\mathcal{M})}{\hat{\sigma}^2} - n + 2 |\mathcal{M}|$$


## Searching The Best Model
```{r}
n_nr_pred <- 16
```

* Exhaustive search over all sub-models might be too expensive
* For $p$ predictors there are $2^p - 1$ sub-models
* With $p=`r n_nr_pred`$, we get $`r 2^n_nr_pred - 1`$ sub-models

$\rightarrow$ step-wise approaches

## Forward Selection
1. Start with smallest sub-model $\mathcal{M}_0$ as current model
2. Include predictor that reduces SSE the most to current model
3. Repeat step 2 until all predictors are chosen

$\rightarrow$ results in sequence $\mathcal{M}_0 \subseteq \mathcal{M}_1 \subseteq \mathcal{M}_2 \subseteq \ldots$ of sub-models

4. Out of sequence of sub-models choose the one with minimal $C_p$


## Backward Selection
1. Start with full model $\mathcal{M}_0$ as the current model
2. Exclude predictor variable that increases SSE the least from current model
3. Repeat step 2 until all predictors are excluded (except for intercept)

$\rightarrow$ results in sequence $\mathcal{M}_0 \supseteq \mathcal{M}_1 \supseteq \mathcal{M}_2 \supseteq \ldots$ of sub-models

4. Out of sequence choose the one with minimal $C_p$


## Considerations
* Whenever possible, choose __backward__ selection, because it leads to better results
* If $p \geq n$, only forward is possible, but then consider LASSO


## Alternative Selection Criteria
* AIC or BIC, requires distributional assumptions. 
* AIC is implemented in `MASS::stepAIC()` 
* Adjusted $R^2$ is a measure of goodness of fit, but sometimes is not conclusive when comparing two models
* Try in exercise 


## Genetic Variation

* Requirement for trait to be considered in breeding goal
* Breeding means improvement of next generation via selection and mating
* Only genetic (additive) components are passed to offspring
* Selection should be based on genetic component of trait
* Selection only possible with genetic variation

$\rightarrow$ genetic variation indicates how good characteristics are passed from parents to offspring

$\rightarrow$ measured by __heritability__ $h^2 = \frac{\sigma_a^2}{\sigma_p^2}$


## Two Traits

```{r preparednormplot, eval=FALSE}
# according to https://sebastiansauer.github.io/normal_curve_ggplot2/
require(ggplot2)
p1 <- ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) + ylab("") +
  scale_y_continuous(breaks = NULL) + 
  theme_bw()
p1
```

```{r twotraitwithwoutgv, echo=FALSE, hook_convert_odg=TRUE, fig_path="odg", out.width="10cm"}
#rmddochelper::use_odg_graphic(ps_path = "odg/twotraitwithwoutgv.odg")
knitr::include_graphics(path = "odg/twotraitwithwoutgv.png")
```
 
 

## Problems

* Genetic components cannot be observed or measured
* Must be estimated from data
* Data are mostly phenotypic

$\rightarrow$ topic of variance components estimation

* Model based, that means connection between phenotypic measure and genetic component are based on certain model

$$p = g + e$$

with $cov(g,e) = 0$ 

* __Goal__: separate variation due to $g$ ($\sigma_a^2$) from phenotypic variation


## Example of Variance Components Separation 

* Estimation of repeatability
* Given repeated measurements of same trait at the same animal
* Repeatability means variation of measurements at the same animal is smaller than variation between measurements at different animals


## Repeatability Plot

```{r repmeasurebulldef, echo=FALSE, message=FALSE, warning=FALSE}
n_nr_bull <- 10
tbl_repmeasure_bull <- tibble::tibble(Bull = c(1:n_nr_bull),
                                      M1 = c(135, 129, 135, 127, 126, 128, 127, 129, 126, 132),
                                      M2 = c(136, 130, 133, 127, 129, 129, 132, 128, 125, 131),
                                      M3 = c(134, 128, 136, 125, 129, 128, 130 ,125, 127, 134))
tbl_rm_bull_ga <- tidyr::gather(tbl_repmeasure_bull, "Measurement", "Height", -Bull)
tbl_rm_bull_ga$Measurement <- as.factor(tbl_rm_bull_ga$Measurement)
tbl_rm_bull_ga$Bull <- as.factor(tbl_rm_bull_ga$Bull)
# ggplot2::ggplot(Bull, Height, data = tbl_rm_bull_ga, colour = Measurement)
library(ggplot2)
ggplot(tbl_rm_bull_ga, aes(x = Bull, y = Height, group = Measurement)) + 
  geom_point(aes(color = Measurement))
```


## Model

\begin{equation}
y_{ij} = \mu + t_i + \epsilon_{ij} \notag
\end{equation}

\begin{tabular}{lll}
where  &  &  \\
       & $y_{ij}$         &  measurement $j$ of animal $i$    \\
       & $\mu$            &  expected value of $y$            \\
       & $t_i$            &  random deviation of $y_{ij}$ from $\mu$ attributed to animal $i$ \\
       & $\epsilon_{ij}$  &  measurement error
\end{tabular}


## Animal Model

* trait of interest as response variable ($y$)
* fixed effects ($b$) as known part of environment
* random animal effect, corresponds to breeding values ($u$)

$$y = Xb + Zu + e$$

with 

* vector $e$ as random residuals and 
* matrices $X$ and $Z$ as design matrices


## Estimates and Predictions

* solution leading to estimates of fixed effects

$$\hat{b} = (X^TV^{-1}X)^{-}X^TV^{-1}y$$

* predictions for random effects

$$\hat{u} = GZ^TV^{-1}(y - X\hat{b})$$

with 

* $G = var(u)$
* $V = var(y)$


## Mixed Model Equations

Equivalent solutions are obtained via

$$\left[
  \begin{array}{lr}
  X^T R^{-1} X  &  X^T R^{-1} Z \\
  Z^T R^{-1} X  &  Z^T R^{-1} Z + G^{-1}
  \end{array}
\right]
\left[
  \begin{array}{c}
  \hat{\beta} \\
  \hat{u}
  \end{array}
\right]
=
\left[
  \begin{array}{c}
  X^T R^{-1} y \\
  Z^T R^{-1} y
  \end{array}
\right]$$

with 

* $G = A * \sigma_u^2$ 

where $A$ is pedigree-based relationship matrix and $\sigma_u^2$ the genetic additive variance


## Single-Step Genomic Breeding Values

* Assume all animals have genotypes

$$y = Xb + Zu + e$$

$$\left[
  \begin{array}{lr}
  X^T R^{-1} X  &  X^T R^{-1} Z \\
  Z^T R^{-1} X  &  Z^T R^{-1} Z + H^{-1}
  \end{array}
\right]
\left[
  \begin{array}{c}
  \hat{\beta} \\
  \hat{u}
  \end{array}
\right]
=
\left[
  \begin{array}{c}
  X^T R^{-1} y \\
  Z^T R^{-1} y
  \end{array}
\right]$$

* $H = A_G * \sigma_u^2$ 

where $A_G$ is the genomic relationship matrix and $\sigma_u^2$ the genetic additive variance
